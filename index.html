<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Index</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-B6CGCXLZWE"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-B6CGCXLZWE');
		</script>

	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<!--
						<h2>Pablo Arias Sarah</h2>
							Hacking social interaction mechanisms with voice/face transformations
						-->	
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Deep dive</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">P. A. S.</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Home</a></li>
							<li><a href="index.html#About">About</a></li>
							<li><a href="index.html#News">News</a></li>
							<li ><a href="index.html#Highlights"> Highlights</a></li>
							<li ><a href="publications.html">Publications</a></li>
							<li ><a href="talks.html">Talks</a></li>
							<li ><a href="ARIAS_CV.pdf">CV</a></li>
							<li ><a href="music.html">Music</a></li>

						</ul>
						<ul class="icons">
							<li><a href="https://twitter.com/pablo_arias_sar" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.linkedin.com/in/pablo-arias-sarah-08a20693/" class="icon brands fa-linkedin-in"><span class="label">Linked-in</span></a></li>
							<li><a href="https://github.com/Pablo-Arias" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>					


				<!-- Main -->
					<div id="main">
						<!-- Intro -->
							<article class="post featured">
								<header class="major", id="About">
									<h3>About <br /></h3>
								</header>

								<div style=font-size:85%>
									<p><span class="image left"><img src="images/profile_rando_test_filtered.jpg" alt="" /></span>Hi! I'm Pablo Arias Sarah, a French/Colombian Postdoctoral researcher working at the University of Glasgow, in the <a href="https://www.gla.ac.uk/schools/psychologyneuroscience/">School of Neuroscience and Psychology</a>, and at <a href="https://www.lunduniversity.lu.se/">Lund University</a>, in the <a href="https://www.lucs.lu.se/home/"><i>Cognitive Science</i></a> lab, in Sweden. Lately, I've been trying to hack human social interaction mechanisms using real time voice/face transformations. To do this, we are developing a new videoconference experimental platform called DuckSoup, which will allow researchers to manipulate participants' voice and face in real time during social interactions.</p>
									
									<p> I hold a a PhD in cognitive science from Sorbonne University, a Master of Engineering in digital technologies and multimedia from Polytech' Nantes, and a Master of Science in acoustics, signal processing and computer science applied to sound, from IRCAM. You can find a complete list of my publications <a href="https://scholar.google.fr/citations?user=6jMFwJQAAAAJ&hl=en&oi=ao">here<a href="https://twitter.com/PabloAriasMusic"> or follow me on twitter</a> to keep up to date with my latest work.
									</p>
								</div>
								
							</article>

						<!-- Featured Post -->
							<article class="post featured">
									<h2 id="News">News</h2>
									<div class="table-wrapper" style='font-size:85%'>
										<table>
											<thead>
												<tr>
													<th>Date</th>
													<th>Description</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Upcoming in 2023</td>
													<td text-align="justify"> We will release our new experimental platform DuckSoup in 2023 üóì. DuckSoup is an opensource videoconference platform that will allow researchers to manipulate participants' facial and vocal attributes in real time during social interactions. If you are interested in collecting big, synchronised & multicultural human social interaction data sets get in touch! üßû‚Äç‚ôÇÔ∏è</td>
												</tr>

												<tr>
													<td>October 2022</td>
													<td text-align="justify">Moving to Scotland to start a new position as Marie Curie Fellow in Glasgow University in the <a href="https://www.gla.ac.uk/schools/psychologyneuroscience/">School of Neuroscience and Psychology</a> department with <a href="https://scholar.google.co.uk/citations?user=cwgW51EAAAAJ&hl=en">Philippe Schyns</a> and <a href="https://scholar.google.co.uk/citations?user=gO2daQsAAAAJ&hl=en"> Rachael Jack</a>. Super psyched! ü§©
													</td>
												</tr>

												<tr>
													<td>June 2022</td>
													<td text-align="justify"> I won a Marie Curie postdoctoral fellowship for my proposal SINA (Studying Social Interactions with Audiovisual Transformations). In collaboration with Rachael Jack, Philippe Schyns (Glasgow University) and Petter Johansson (Lund Unviersity)! üí£</td>
												</tr>

												<tr>
													<td>June 2021</td>
													<td text-align="justify">We won the Sorbonne Univeristy "Emergence" call with our REVOLT (Revealing human bias with real time vocal deep fakes) proposal, in collaboration with Nicolas Obin (SU) üí•. </td>
												</tr>

												<tr>
													<td>Sept 2019</td>
													<td text-align="justify">I'm moving to a new postdoctoral position at Lund University Cognitive Science in Sweden to work with Petter Johannsson and Lars Hall in the Choice Blindeness lab! We aim to create unprecedented methodological tools and experimental paradigms to study human social interaction mechanisms. </td>
												</tr>												

												<tr>
													<td>Dec 2018</td>
													<td text-align="justify">Defended my PhD thesis entitled <a href="https://hal.archives-ouvertes.fr/tel-02010161/file/PhD%20Arias.pdf"> The cognition of auditory smiles: a computational approach"</a>, which was evaluated by an inspring jury composed of <a href="https://scholar.google.fr/citations?user=XWdplJkAAAAJ&hl=en&oi=ao">Tecumseh Fitch</a> (Univ. Viena), <a href="https://scholar.google.fr/citations?user=XWdplJkAAAAJ&hl=en&oi=ao">Rachael Jack</a> (Univ. Glasgow), <a href="https://scholar.google.fr/citations?user=n9ZNrsEAAAAJ&hl=en&oi=ao">Catherine Pelachaud</a> (Sorbonne University), <a href="https://scholar.google.fr/citations?user=qKynVZ0AAAAJ&hl=en&oi=ao">Martine Gavaret</a>> (Paris Descartes),  <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Julie Grezes</a>  and <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Pascal Belin</a> (Univ. Aix Marseille), <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao"> Patrick Susini</a> (IRCAM) and <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Jean-Julien Aucouturier</a> (CNRS).</td>
												</tr>
											</tbody>
										</table>
									</div>
								
							</article>	

						<!-- Posts -->
							<section class="post">
								<header class="major", id="Highlights">
									<h2>Highlights </h2>
								</header>
							</section>								
							
							<section class="posts">
								
								<!-- Facial mimicry in the congenitally blind -->
								<article>
									<header>
										<span class="date">December, 2021</span>
										<h4><a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf">Facial mimicry in the congenitally blind<br /></a></h4>
									</header>

									<a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf" class="image fit"><img src="images/main_figure_CB_v1.jpg" alt="" /></a>

									<div style='font-size:85%'>
										<p> We have a new article out in <i>Current Biology</i>! We show that congenitally blind individuals facially imitate smiles heard in speech despite having never seen a facial expression. This demonstrates that the development of facial mimicry does not depend on visual learning and that imitation is not a mere visuo-motor process but a flexible mechanism deployed across sensory inputs. 

										Check the full article <a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf">here</a>. Or check <a href="https://twitter.com/PabloAriasMusic/status/1453637734489284608">this</a> twitter thread explaining the findings.
										</p>
									</div>

								</article>

								<!-- Beyond correlation: acoustic transformation methods for the experimental study of emotional voice and speech -->
								<article>
									<header>
										<span class="date">January, 2021</span>
										<h4><a href="https://hal.archives-ouvertes.fr/hal-02907502/document">Beyond correlation: acoustic transformation methods for the experimental study of emotional voice and speech<br /></a></h4>
									</header>
									<a href="https://hal.archives-ouvertes.fr/hal-02907502/document" class="image fit"><img src="images/emotion_review.jpg" alt="" /></a>
									
										<div style='font-size:85%'>
											<p> We have a new article out in <i>Emotion Review</i>! In this article we present the methodological advantages of using stimulus manipulation techniques for the experimental study of emotions. We give several examples using such computational models to uncover cognitive mechanisms, and argue that such stimulus manipulation techniques can allow researchers to make causal inferences between stimulus features and participant's behavioral, physiological and neural responses. 
											</p>
										<div style='font-size:85%'>
								</article>

								<!-- Auditory smiles trigger unconscious facial imitation -->
								<article>
									<header>
										<span class="date">April, 2018</span>
										<h4><a href="https://www.cell.com/current-biology/pdf/S0960-9822(18)30752-8.pdf">Auditory smiles trigger unconscious facial imitation<br /></a></h4>
									</header>
									<a href="https://www.cell.com/current-biology/pdf/S0960-9822(18)30752-8.pdf" class="image fit"><img src="images/auditory_smiles_trigger.jpg" alt="" /></a>
										<div style='font-size:85%'>
											<p>
											We have a new article out in <i>Current Biology</i> ü•≥ !! In this article we modeled the auditory consequences of smiles in speech and showed that such auditory smiles can trigger facial imitation in listeners even in the absence of visual cues. Interestingly, these reactions occur even when participants do not explicitly detect the smiles.
											</p>
										</div>
								</article>

								<article>
									<header>
										<span class="date">January, 2018</span>
										<h4><a href="https://www.researchgate.net/profile/Emmanuel-Ponsot/publication/322609047_Uncovering_mental_representations_of_smiled_speech_using_reverse_correlation/links/5a659c69a6fdccb61c583953/Uncovering-mental-representations-of-smiled-speech-using-reverse-correlation.pdf">Uncovering mental representations of smiled speech using reverse correlation.</h4>
									</header>
									<a href="https://www.researchgate.net/profile/Emmanuel-Ponsot/publication/322609047_Uncovering_mental_representations_of_smiled_speech_using_reverse_correlation/links/5a659c69a6fdccb61c583953/Uncovering-mental-representations-of-smiled-speech-using-reverse-correlation.pdf" class="image fit"><img src="images/jasa_el.jpg" alt="" /></a>
										<div style='font-size:85%'>
											<p>
											New article out in <i>JASA-EL</i>! We uncovered the spectral cues underlying the perceptual processing of smiles in speech using reverse correlation. The analyses revealed that listeners rely on robust spectral representations that specifically encode vowel‚Äôs formants. These findings demonstrate the causal role played by formants in the perception of smiles and present a novel method to estimate the spectral bases of high-level (e.g., emotional/social/paralinguistic) speech representations.
											</p>
										</div>
								</article>
							</section>

					</div>


				<!-- Footer -->
					<!-- First Column -->
					<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email</h3>
								<p><a href="#">pablo[dot]arias[dot]sar(At]gmail.com</a></p>
							</section>
						</section>

						<!-- Second Column -->
						<section class="split contact">
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://twitter.com/pablo_arias_sar" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="https://www.linkedin.com/in/pablo-arias-sarah-08a20693/" class="icon brands fa-linkedin-in"><span class="label">Linked-in</span></a></li>
									<li><a href="https://github.com/Pablo-Arias/STIM" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>