<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Index</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-B6CGCXLZWE"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-B6CGCXLZWE');
		</script>

	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<!--
						<h2>Nadia Guerouaou</h2>
							Manipulating emotions in voice with algorithms, from ethics to therapy
						-->	
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Deep dive</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">P. A. S.</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li class="active"><a href="index.html">Home</a></li>
							<li><a href="index.html#About">About</a></li>
							<li><a href="index.html#News">News</a></li>
							<li ><a href="index.html#Highlights"> Highlights</a></li>
							<li ><a href="publications.html">Publications</a></li>
							<li ><a href="talks.html">Talks</a></li>
							<li ><a href="GUEROUAOU_CV.pdf">CV</a></li>
							<li ><a href="teaching.html">Teaching</a></li>

						</ul>
						<ul class="icons">
							<li><a href="https://twitter.com/GuerouaouN" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.linkedin.com/in/nadia-guerouaou-004b28224/" class="icon brands fa-linkedin-in"><span class="label">Linked-in</span></a></li>
							<li><a href="https://github.com/nadiaguerouaou" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>					


				<!-- Main -->
					<div id="main">
						<!-- Intro -->
							<article class="post featured">
								<header class="major", id="About">
									<h3>About <br /></h3>
								</header>

								<div style=font-size:85%>
									<p><span class="image left"><img src="images/profile_rando_test_filtered.jpg" alt="" /></span>Hi! I'm Pablo Arias Sarah, a French/Colombian Postdoctoral researcher working at the University of Glasgow, in the <a href="https://www.gla.ac.uk/schools/psychologyneuroscience/">School of Neuroscience and Psychology</a>, and at <a href="https://www.lunduniversity.lu.se/">Lund University</a>, in the <a href="https://www.lucs.lu.se/home/"><i>Cognitive Science</i></a> lab, in Sweden. Lately, I've been trying to hack human social interaction mechanisms using real time voice/face transformations. To do this, we are developing a new videoconference experimental platform called DuckSoup, which will allow researchers to manipulate participants' voice and face in real time during social interactions.</p>
									
									<p> I hold a a PhD in cognitive science from Sorbonne University, a Master of Engineering in digital technologies and multimedia from Polytech' Nantes, and a Master of Science in acoustics, signal processing and computer science applied to sound, from IRCAM. You can find a complete list of my publications <a href="https://scholar.google.fr/citations?user=6jMFwJQAAAAJ&hl=en&oi=ao">here<a href="https://twitter.com/PabloAriasMusic"> or follow me on twitter</a> to keep up to date with my latest work.
									</p>
								</div>
								
							</article>

						<!-- Featured Post -->
							<article class="post featured">
									<h2 id="News">News</h2>
									<div class="table-wrapper" style='font-size:85%'>
										<table>
											<thead>
												<tr>
													<th>Date</th>
													<th>Description</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Upcoming in 2023</td>
													<td text-align="justify"> We will release our new experimental platform DuckSoup in 2023 üóì. DuckSoup is an opensource videoconference platform allowing researchers to manipulate participants' facial and vocal attributes in real time during social interactions. If you are interested in collecting large, synchronised & multicultural human social interaction data sets get in touch! üßû‚Äç‚ôÇÔ∏è</td>
												</tr>

												<tr>
													<td>November 2022</td>
													<td text-align="justify"> We won the VR grant application from the Swedish Research Council to develop our new platform DuckSoup! 
													</td>
												</tr>

												<tr>
													<td>October 2022</td>
													<td text-align="justify">Moving to Scotland to start a new position as Marie Curie Fellow in Glasgow University in the <a href="https://www.gla.ac.uk/schools/psychologyneuroscience/">School of Neuroscience and Psychology</a> department with <a href="https://scholar.google.co.uk/citations?user=cwgW51EAAAAJ&hl=en">Philippe Schyns</a> and <a href="https://scholar.google.co.uk/citations?user=gO2daQsAAAAJ&hl=en"> Rachael Jack</a>. Super psyched! ü§©
													</td>
												</tr>

												<tr>
													<td>June 2022</td>
													<td text-align="justify"> I won a Marie Curie postdoctoral fellowship for my proposal SINA (Studying Social Interactions with Audiovisual Transformations). In collaboration with Rachael Jack, Philippe Schyns (Glasgow University) and Petter Johansson (Lund Unviersity)! üí£</td>
												</tr>

												<tr>
													<td>June 2021</td>
													<td text-align="justify">We won the Sorbonne Univeristy "Emergence" call with our REVOLT (Revealing human bias with real time vocal deep fakes) proposal, in collaboration with Nicolas Obin (SU) üí•. </td>
												</tr>

												<tr>
													<td>Sept 2019</td>
													<td text-align="justify">I'm starting a new postdoctoral position at Lund University Cognitive Science in Sweden to work with Petter Johannsson and Lars Hall in the Choice Blindeness lab! We aim to create unprecedented methodological tools to study human social interaction mechanisms. </td>
												</tr>												

												<tr>
													<td>Dec 2018</td>
													<td text-align="justify">Defended my PhD thesis entitled <a href="https://hal.archives-ouvertes.fr/tel-02010161/file/PhD%20Arias.pdf"> The cognition of auditory smiles: a computational approach"</a>, which was evaluated by an inspring jury composed of <a href="https://scholar.google.fr/citations?user=XWdplJkAAAAJ&hl=en&oi=ao">Tecumseh Fitch</a> (Univ. Viena), <a href="https://scholar.google.fr/citations?user=XWdplJkAAAAJ&hl=en&oi=ao">Rachael Jack</a> (Univ. Glasgow), <a href="https://scholar.google.fr/citations?user=n9ZNrsEAAAAJ&hl=en&oi=ao">Catherine Pelachaud</a> (Sorbonne University), <a href="https://scholar.google.fr/citations?user=qKynVZ0AAAAJ&hl=en&oi=ao">Martine Gavaret</a>> (Paris Descartes),  <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Julie Grezes</a>  and <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Pascal Belin</a> (Univ. Aix Marseille), <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao"> Patrick Susini</a> (IRCAM) and <a href="https://scholar.google.fr/citations?user=PjXi-vYAAAAJ&hl=en&oi=ao">Jean-Julien Aucouturier</a> (CNRS).</td>
												</tr>
											</tbody>
										</table>
									</div>
								
							</article>	

						<!-- Posts -->
							<section class="post">
								<header class="major", id="Highlights">
									<h2>Highlights </h2>
								</header>
							</section>								
							
							<section class="posts">
								
								<!-- Facial mimicry in the congenitally blind -->
								<article>
									<header>
										<span class="date">December, 2021</span>
										<h4><a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf">Facial mimicry in the congenitally blind<br /></a></h4>
									</header>

									<a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf" class="image fit"><img src="images/main_figure_CB_v1.jpg" alt="" /></a>

									<div style='font-size:85%'>
										<p> We have a new article out in <i>Current Biology</i>! We show that congenitally blind individuals facially imitate smiles heard in speech despite having never seen a facial expression. This demonstrates that the development of facial mimicry does not depend on visual learning and that imitation is not a mere visuo-motor process but a flexible mechanism deployed across sensory inputs. 

										Check the full article <a href="https://neuro-team-femto.github.io/articles/2021/Arias_Current_Biology_2021.pdf">here</a>. Or check <a href="https://twitter.com/PabloAriasMusic/status/1453637734489284608">this</a> twitter thread explaining the findings.
										</p>
									</div>

								</article>

								<!-- Rendre sa voix plus souriante: deepfakes et filtres vocaux √©motionnels -->
								<article>
									<header>
										<span class="date">July, 2022</span>
										<h4><a href="https://aoc.media/analyse/2022/07/05/rendre-sa-voix-plus-souriante-deepfakes-et-filtres-vocaux-emotionnels/">Rendre sa voix plus souriante: deepfakes et filtres vocaux √©motionnels<br /></a></h4>
									</header>
									<a href="https://aoc.media/analyse/2022/07/05/rendre-sa-voix-plus-souriante-deepfakes-et-filtres-vocaux-emotionnels/" class="image fit"><img src="images/emotion_review.jpg" alt="" /></a>
									
										<div style='font-size:85%'>
											<p> We have a new article out in <i>Emotion Review</i>! In this article we present the methodological advantages of using stimulus manipulation techniques for the experimental study of emotions. We give several examples using such computational models to uncover cognitive mechanisms, and argue that such stimulus manipulation techniques can allow researchers to make causal inferences between stimulus features and participant's behavioral, physiological and neural responses. 
											</p>
										<div style='font-size:85%'>
								</article>

								<!-- L'√©tonnante acceptabilit√© des deepfakes -->
								<article>
									<header>
										<span class="date">January, 2022</span>
										<h4><a href="https://www.cell.com/current-biology/pdf/S0960-9822(18)30752-8.pdf">Auditory smiles trigger unconscious facial imitation<br /></a></h4>
									</header>
									<a href="https://www.cell.com/current-biology/pdf/S0960-9822(18)30752-8.pdf" class="image fit"><img src="images/auditory_smiles_trigger.jpg" alt="" /></a>
										<div style='font-size:85%'>
											<p>
											Our post on ‚ÄúThe surprising acceptability of deepfakes‚Äù has just been published in <a href="https://www.liberation.fr/idees-et-debats/tribunes/letonnante-acceptabilite-des-deep-fakes-20220106_E4MZA6GDJREARN25BBCA7JXH3M/">Lib√©ration</a>  & <a href="https://lejournal.cnrs.fr/billets/letonnante-acceptabilite-des-deep-fake">CNRS Le journal</a>. So slad we have been given the opportunity to talk about this topic of high societal interest üòÉ. Also available in english <a href="https://news.cnrs.fr/opinions/the-unforeseen-acceptance-of-deepfakes">here</a>.  
											</p>
										</div>
								</article>

								<!-- The shallow of your smile: the ethics of expressive vocal deep-fakes -->
								<article>
									<header>
										<span class="date">January, 2018</span>
										<h4><a href="https://doi.org/10.1098/rstb.2021.0083">The shallow of your smile: the ethics of expressive vocal deep-fakes.</h4>
									</header>
									<a href="https://doi.org/10.1098/rstb.2021.0083" class="image fit"><img src="images/jasa_el.jpg" alt="" /></a>
										<div style='font-size:85%'>
											<p>
											Our article has been published in <i>Philosophical Transactions B of the Royal Society</i>! in just a few years, with the proliferation of ‚Äúdeep-fake‚Äù technologies, we‚Äôve come to a situation where it is difficult to trust whether the smiles, laughs and frowns of our conversation partners are genuine or algorithmically modulated. Inspired by the methodology of experimental ethics (autonomous vehicules...), we have conducted a study to evaluate the moral acceptability of 24 Black-mirroresque scenarios describing potential applications of vocal filters <a href="https://twitter.com/GuerouaouN/status/1469337128698093581?s=20&t=qIuGyYeG3LaGNQ5f2lqJ9Q">Here</a> the twitter thread explaining the study and findings.
											</p>
										</div>
								</article>
							</section>

					</div>


				<!-- Footer -->
					<!-- First Column -->
					<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email</h3>
								<p><a href="#">n[dot]guerouaou(At]gmail.com</a></p>
							</section>
						</section>

						<!-- Second Column -->
						<section class="split contact">
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://twitter.com/GuerouaouN" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
									<li><a href="https://linkedin.com/in/nadia-guerouaou-004b28224" class="icon brands fa-linkedin-in"><span class="label">Linked-in</span></a></li>									
									<li><a href="https://github.com/nadiaguerouaou" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
